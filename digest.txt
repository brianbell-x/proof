Directory structure:
â””â”€â”€ Prover/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ core/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ agent.py
    â”‚   â””â”€â”€ __pycache__/
    â”œâ”€â”€ docs/
    â”‚   â””â”€â”€ openrouter.md
    â”œâ”€â”€ logs/
    â”œâ”€â”€ prompts/
    â”‚   â”œâ”€â”€ grading_prompt.md
    â”‚   â”œâ”€â”€ master.md
    â”‚   â””â”€â”€ tool_prompt.md
    â”œâ”€â”€ temp/
    â”‚   â””â”€â”€ todo
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ test_agent.py
    â”‚   â”œâ”€â”€ test_ai_grading.py
    â”‚   â”œâ”€â”€ test_claims.py
    â”‚   â”œâ”€â”€ test_tools.py
    â”‚   â”œâ”€â”€ __pycache__/
    â”‚   â””â”€â”€ fixtures/
    â”‚       â””â”€â”€ claims.json
    â””â”€â”€ tools/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ python_repl.py
        â”œâ”€â”€ search.py
        â””â”€â”€ __pycache__/

================================================
File: README.md
================================================
# Prover Agent, as a tool? (haven't decided on a name)

A real time FUD destruction engine that stress tests claims through rigorous logical analysis and evidence gathering. Not a fact checker, a proof checker. Feed it any claim and get a transparent proof tree showing logical derivation or exposing shaky foundations. Built on falsification first skepticism: it tries to break claims before defending them, shows its work, cites sources, and admits when it doesn't know.

## Quick Start

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment**:
   ```bash
   cp .env.example .env
   # Edit .env with your OPENROUTER_API_KEY
   ```

3. **Run the agent**:
   ```bash
   python -m core.agent
   ```
4. **Type a claim and see the proof tree**

## The Story:

Frustrated with social media. Every day feeds full of confident claims, some true, some false, most somewhere in between. Grok does quick searches (can be poisoned). Community Notes are gamed. What if we could go deeper? Build something that doesn't just cite sources but actually derives truth from first principles?

The vision: real time FUD destruction engine. Ask any claim, get a proof tree in 5 seconds showing logical derivation or exposing the foundation as sand. Not a fact checker. A proof checker.

Starting simple. Just a CLI that takes text input, outputs proof tree. No UI, no polish. Core question: can AI construct rigorous proofs?

Using Grok. What better model than the truth seeker itself? Made the master prompt. This is 80% of the product. Need structured JSON: claim, assumptions, derivation steps, falsifiable tests, verdict.

First runs are weird. Model outputs fine but feels like it's justifying claims, not evaluating them. Building proofs for the claim, not stress testing them. Problem...

Made a few changes but still noticing bias. Model wants to prove claims, not test them. Looking at the prompt language: "Your purpose is to construct rigorous, verifiable proof trees for claims."

That word construct is the problem. Implies building something for the claim, not against it. Model acting like a defense attorney. Should be a skeptical scientist.

Rewriting the whole prompt. Instead of construct proofs, it's stress test claims. Instead of seeking supporting evidence first, seek falsification. Core principle: falsification first.

New prompt tells the model: Before seeking supporting evidence, identify what would disprove the claim. Your goal is to find flaws, not justify assertions.

Gonna remove confidence scoring entirely. The verdicts themselves are enough: PROVEN, DISPROVEN, UNSUPPORTED, UNVERIFIABLE. These say something about the claim, not about how the model "feels" about it.

Made test_claims.py with some sample claims. Some true, some false, some unsubstantiated. But static tests are annoying with LLMs. Same claim gives different reasoning each time. Can't just check if verdict matches expected.

Had an idea. What if I use another AI to grade the analysis? Built test_ai_grading.py. Instead of checking if verdicts match, it checks if the reasoning makes sense. Does the evidence actually support the verdict? Is the logic coherent? Are the assumptions reasonable?

This is way better. Flexible validation that can handle the non deterministic nature of LLMs.

The verdict categories kept changing. Started with vague stuff like DERIVED FROM FIRST PRINCIPLES vs MATHEMATICALLY UNSOUND. Too ambiguous.

Refined it to four clear categories with a simple decision tree. Can it be tested? No means UNVERIFIABLE. Does evidence prove it? Yes means PROVEN. Does evidence disprove it? Yes means DISPROVEN. Otherwise UNSUPPORTED.

Much clearer now. The model has explicit criteria for each verdict. Less wiggle room.

Been thinking about what this all means. The philosophy that's emerging: skepticism over justification. Find flaws, don't defend claims. Evidence over fake confidence. Show your work. Falsification over verification. Try to break things. Tools over training data. Real evidence matters. Transparency. Show every step.

It's not perfect. Not omniscient. But it's honest. Shows its work. Cites sources. Admits when it doesn't know.

Still just a proof of concept. The real vision is bigger. Imagine X with Prove button on every post. Background system building proof trees for viral claims. Make truth seeking as easy as sharing misinformation.

Considering the next iteration to make this a tool (like agent as a tool) to seperate concerns from a chat model.

After testing this a bit more... I need the date and time to be injected into the context. 


================================================
File: .env.example
================================================
OPENROUTER_API_KEY=your_openrouter_api_key_here


================================================
File: core/__init__.py
================================================



================================================
File: core/agent.py
================================================
"""
This module implements an agentic reasoning system that can use tools to gather evidence
and perform calculations while constructing rigorous proofs for claims.
"""

import os
import json
import logging
from typing import Dict, Any
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

from tools.search import WebSearchTool, get_tool_schema as get_search_schema
from tools.python_repl import PythonREPLTool, get_tool_schema as get_python_schema

load_dotenv()
api_key = os.getenv("OPENROUTER_API_KEY")

if not api_key:
    raise ValueError("OPENROUTER_API_KEY not found in .env file or environment variables.")

log_dir = 'logs'
log_file = os.path.join(log_dir, 'prover.log')

try:
    os.makedirs(log_dir, exist_ok=True)
    file_handler = logging.FileHandler(log_file)
except (OSError, PermissionError) as e:
    print(f"Warning: Could not create log file {log_file}: {e}. Logging to console only.")
    file_handler = None

handlers = [logging.StreamHandler()]
if file_handler:
    handlers.append(file_handler)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=handlers
)

# Configure console verbosity based on PROVER_VERBOSE env var
console_level = logging.WARNING
if os.getenv("PROVER_VERBOSE", "0").lower() in ("1", "true", "yes"):
    console_level = logging.INFO
logging.getLogger().handlers[0].setLevel(console_level)

# Console log truncation for readability (file logs remain full)
PROVER_LOG_MAX_CHARS = int(os.getenv("PROVER_LOG_MAX_CHARS", "0"))  # 0 = no truncation

logger = logging.getLogger(__name__)

class ProverAgent:
    """Agent that constructs rigorous proofs using tools for evidence gathering."""

    def __init__(self, api_key: str, model: str = "x-ai/grok-4-fast"):
        self.api_key = api_key
        self.model = model
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )

        self.tools = {}
        self.tools["web_search"] = WebSearchTool(api_key)
        self.tools["python_execute"] = PythonREPLTool()

        self.master_prompt = self._load_prompt("prompts/master.md")
        self.tool_prompt = self._load_prompt("prompts/tool_prompt.md")
        current_date = datetime.now().strftime("%Y-%m-%d")
        self.system_prompt = f"Current date: {current_date}\n\n" + self.master_prompt + "\n\n" + self.tool_prompt

        self.tool_schemas = []
        if ":online" not in self.model:
            self.tool_schemas.append(get_search_schema())
        self.tool_schemas.append(get_python_schema())

    def _load_prompt(self, path: str) -> str:
        try:
            with open(path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"Prompt file not found: {path}")
            return ""

    def _execute_tool(self, tool_call) -> Dict[str, Any]:
        import time
        start_time = time.time()

        try:
            tool_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)

            logger.info(f"[TOOL CALL REQUEST][id={tool_call.id}][name={tool_name}] args={json.dumps(arguments)}")

            if tool_name not in self.tools:
                result = {
                    "error": f"Unknown tool: {tool_name}",
                    "tool_call_id": tool_call.id
                }
            else:
                tool = self.tools[tool_name]

                if tool_name == "web_search" and tool_name in self.tools:
                    result = tool.search(**arguments)
                elif tool_name == "python_execute":
                    result = tool.execute(**arguments)
                else:
                    result = {"error": f"Tool {tool_name} not implemented or not available for this model"}

                result["tool_call_id"] = tool_call.id
                result["tool_name"] = tool_name

            duration = time.time() - start_time
            result_payload = json.dumps(result)
            if PROVER_LOG_MAX_CHARS > 0 and len(result_payload) > PROVER_LOG_MAX_CHARS:
                truncated_payload = result_payload[:PROVER_LOG_MAX_CHARS] + "... (truncated)"
                logger.info(f"[TOOL RESULT][id={tool_call.id}][name={tool_name}][duration={duration:.3f}s] result={truncated_payload}")
            else:
                logger.info(f"[TOOL RESULT][id={tool_call.id}][name={tool_name}][duration={duration:.3f}s] result={result_payload}")

            return result

        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"[TOOL RESULT][id={getattr(tool_call, 'id', 'unknown')}][name={getattr(tool_call.function, 'name', 'unknown') if hasattr(tool_call, 'function') else 'unknown'}][duration={duration:.3f}s] error={str(e)}")
            return {
                "error": str(e),
                "tool_call_id": getattr(tool_call, "id", None),
                "tool_name": getattr(tool_call.function, "name", None) if hasattr(tool_call, "function") else None
            }

    def prove_claim(self, claim: str, max_iterations: int = 5) -> Dict[str, Any]:
        messages = [
            {
                "role": "system",
                "content": self.system_prompt
            },
            {
                "role": "user",
                "content": claim
            }
        ]

        iteration = 0
        final_result = None
        tools_used = set()

        while iteration < max_iterations:
            iteration += 1
            logger.info(f"Starting iteration {iteration}")

            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    tools=self.tool_schemas,
                    tool_choice="auto"
                )

                message = response.choices[0].message
                message_dict = message.model_dump() if hasattr(message, 'model_dump') else message
                messages.append(message_dict)

                # Log model output with full details
                finish_reason = getattr(response.choices[0], 'finish_reason', 'unknown')
                model_used = getattr(response, 'model', self.model)
                content = message.content or ""
                tool_calls_info = ""
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    tool_calls_list = []
                    for tc in message.tool_calls:
                        tool_calls_list.append(f"id={tc.id} name={tc.function.name} args={tc.function.arguments}")
                    tool_calls_info = f" tool_calls=[{', '.join(tool_calls_list)}]"

                if "verdict" in content:
                    logger.info(f"[MODEL OUTPUT][iter={iteration}][finish_reason={finish_reason}][model={model_used}] content={content}{tool_calls_info}")
                else:
                    logger.info(f"[MODEL OUTPUT][iter={iteration}][finish_reason={finish_reason}][model={model_used}] content={content}{tool_calls_info}")

                try:
                    result = json.loads(content) if content else {}
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"[MODEL OUTPUT][parse_error] Failed to parse JSON response: {content}")
                    result = {"error": "Invalid JSON response", "raw_content": content}

                if "verdict" in result:
                    final_result = result
                    logger.info("[FINAL RESULT] Verdict reached")
                    break

                if hasattr(message, 'tool_calls') and message.tool_calls:
                    logger.info(f"Processing {len(message.tool_calls)} tool calls")

                    for tool_call in message.tool_calls:
                        tools_used.add(tool_call.function.name)
                        tool_result = self._execute_tool(tool_call)

                        tool_message = {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(tool_result)
                        }
                        messages.append(tool_message)
                        logger.info(f"[TOOL RESULT SENT][id={tool_call.id}] message={json.dumps(tool_message)}")

                else:
                    logger.info("No tool calls in response, continuing...")

            except Exception as e:
                logger.error(f"Error in iteration {iteration}: {e}")
                return {
                    "error": str(e),
                    "claim": claim,
                    "iterations_completed": iteration
                }

        if final_result:
            final_result["iterations_used"] = iteration
            final_result["tools_used"] = list(tools_used)
            logger.info(f"[FINAL RESULT] {json.dumps(final_result)}")
            return final_result
        else:
            partial = {
                "error": "Maximum iterations reached without reaching a verdict",
                "claim": claim,
                "iterations_completed": iteration,
                "partial_result": result if 'result' in locals() else None,
                "tools_used": list(tools_used)
            }
            logger.info(f"[FINAL RESULT] {json.dumps(partial)}")
            return partial


def main():
    print("Proof Agent")
    print("====================")
    print("Enter a claim to analyze (or 'quit' to exit):")
    print("Set PROVER_VERBOSE=1 to see detailed logs in console.")
    print("Set PROVER_SHOW_FULL=1 to print full JSON results to console.")

    agent = ProverAgent(api_key)

    while True:
        try:
            user_input = input("\nClaim: ").strip()

            if user_input.lower() in ['quit']:
                logger.info("User requested exit")
                break

            if not user_input:
                continue

            logger.info(f"Analyzing claim: {user_input}")

            result = agent.prove_claim(user_input)

            logger.info("Analysis complete")

            # Pretty CLI summary
            if "error" in result:
                print(f"\nâŒ Error: {result['error']}")
                if "partial_result" in result:
                    print("Partial result available in logs.")
            else:
                verdict = result.get("verdict", "UNKNOWN")
                confidence = result.get("confidence", "UNKNOWN")
                reason = result.get("reasoning_summary", "No summary available")
                iterations = result.get("iterations_used", "?")
                tools = result.get("tools_used", [])

                verdict_emoji = {"PROVEN": "âœ…", "DISPROVEN": "âŒ", "UNSUPPORTED": "â“", "UNVERIFIABLE": "ðŸ¤·"}.get(verdict, "â“")

                print(f"\n{verdict_emoji} Verdict: {verdict}")
                print(f"   Confidence: {confidence}")
                print(f"   Reason: {reason}")
                print(f"   Iterations: {iterations}")
                print(f"   Tools used: {', '.join(tools) if tools else 'None'}")

            # Optional full JSON output
            if os.getenv("PROVER_SHOW_FULL", "0").lower() in ("1", "true", "yes"):
                print("\nFull result:")
                print(json.dumps(result, indent=2))

        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt, exiting...")
            break
        except Exception as e:
            logger.error(f"An error occurred in main loop: {e}")


if __name__ == "__main__":
    main()



================================================
File: docs/openrouter.md
================================================
OpenRouter provides a unified API that gives you access to hundreds of AI models through a single endpoint, while automatically handling fallbacks and selecting the most cost-effective options. Get started with just a few lines of code using your preferred SDK or framework.

> **Tip:**
  Looking for information about free models and rate limits? Please see the [FAQ](/docs/faq#how-are-rate-limits-calculated)

In the examples below, the OpenRouter-specific headers are optional. Setting them allows your app to appear on the OpenRouter leaderboards. For detailed information about app attribution, see our [App Attribution guide](/docs/app-attribution).

## Using the OpenRouter API directly

> **Tip:**
  You can use the interactive [Request Builder](/request-builder) to generate OpenRouter API requests in the language of your choice.

```python
import requests
import json

response = requests.post(
  url="https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": "Bearer <OPENROUTER_API_KEY>",
    "HTTP-Referer": "<YOUR_SITE_URL>",
    "X-Title": "<YOUR_SITE_NAME>",
  },
  data=json.dumps({
    "model": "openai/gpt-4o",
    "messages": [
      {
        "role": "user",
        "content": "What is the meaning of life?"
      }
    ]
  })
)
```

```shell
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
}'
```

## Using the OpenAI SDK

```python
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>",
    "X-Title": "<YOUR_SITE_NAME>",
  },
  model="openai/gpt-4o",
  messages=[
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
)

print(completion.choices[0].message.content)
```

The API also supports [streaming](/docs/api-reference/streaming).

## Using third-party SDKs

For information about using third-party SDKs and frameworks with OpenRouter, please [see our frameworks documentation.](/docs/community/frameworks-and-integrations-overview)

OpenRouter provides two options for model routing.

## Auto Router

The [Auto Router](https://openrouter.ai/openrouter/auto), a special model ID that you can use to choose between selected high-quality models based on your prompt, powered by [NotDiamond](https://www.notdiamond.ai/).

```python
import requests
import json

response = requests.post(
  url="https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": "Bearer <OPENROUTER_API_KEY>",
    "Content-Type": "application/json",
  },
  data=json.dumps({
    "model": "openrouter/auto",
    "messages": [
      {
        "role": "user",
        "content": "What is the meaning of life?"
      }
    ]
  })
)

data = response.json()
print(data['choices'][0]['message']['content'])
```

The resulting generation will have `model` set to the model that was used.

## The `models` parameter

The `models` parameter lets you automatically try other models if the primary model's providers are down, rate-limited, or refuse to reply due to content moderation.

```python
import requests
import json

response = requests.post(
  url="https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": "Bearer <OPENROUTER_API_KEY>",
    "Content-Type": "application/json",
  },
  data=json.dumps({
    "models": ["anthropic/claude-3.5-sonnet", "gryphe/mythomax-l2-13b"],
    "messages": [
      {
        "role": "user",
        "content": "What is the meaning of life?"
      }
    ]
  })
)

data = response.json()
print(data['choices'][0]['message']['content'])
```

If the model you selected returns an error, OpenRouter will try to use the fallback model instead. If the fallback model is down or returns an error, OpenRouter will return that error.

By default, any error can trigger the use of a fallback model, including context length validation errors, moderation flags for filtered models, rate-limiting, and downtime.

Requests are priced using the model that was ultimately used, which will be returned in the `model` attribute of the response body.

## Using with OpenAI SDK

To use the `models` array with the OpenAI SDK, include it in the `extra_body` parameter. In the example below, gpt-4o will be tried first, and the `models` array will be tried in order as fallbacks.

```python
from openai import OpenAI

openai_client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=<OPENROUTER_API_KEY>,
)

completion = openai_client.chat.completions.create(
    model="openai/gpt-4o",
    extra_body={
        "models": ["anthropic/claude-3.5-sonnet", "gryphe/mythomax-l2-13b"],
    },
    messages=[
        {
            "role": "user",
            "content": "What is the meaning of life?"
        }
    ]
)

print(completion.choices[0].message.content)
```

OpenRouter routes requests to the best available providers for your model. By default, [requests are load balanced](#price-based-load-balancing-default-strategy) across the top providers to maximize uptime.

You can customize how your requests are routed using the `provider` object in the request body for [Chat Completions](/docs/api-reference/chat-completion) and [Completions](/docs/api-reference/completion).

> **Tip:**
  For a complete list of valid provider names to use in the API, see the [full
  provider schema](#json-schema-for-provider-preferences).

The `provider` object can contain the following fields:

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `order` | string[] | - | List of provider slugs to try in order (e.g. `["anthropic", "openai"]`). [Learn more](#ordering-specific-providers) |
| `allow_fallbacks` | boolean | `true` | Whether to allow backup providers when the primary is unavailable. [Learn more](#disabling-fallbacks) |
| `require_parameters` | boolean | `false` | Only use providers that support all parameters in your request. [Learn more](#requiring-providers-to-support-all-parameters-beta) |
| `data_collection` | "allow" \| "deny" | "allow" | Control whether to use providers that may store data. [Learn more](#requiring-providers-to-comply-with-data-policies) |
| `zdr` | boolean | - | Restrict routing to only ZDR (Zero Data Retention) endpoints. [Learn more](#zero-data-retention-enforcement) |
| `enforce_distillable_text` | boolean | - | Restrict routing to only models that allow text distillation. [Learn more](#distillable-text-enforcement) |
| `only` | string[] | - | List of provider slugs to allow for this request. [Learn more](#allowing-only-specific-providers) |
| `ignore` | string[] | - | List of provider slugs to skip for this request. [Learn more](#ignoring-providers) |
| `quantizations` | string[] | - | List of quantization levels to filter by (e.g. `["int4", "int8"]`). [Learn more](#quantization) |
| `sort` | string | - | Sort providers by price or throughput. (e.g. `"price"` or `"throughput"`). [Learn more](#provider-sorting) |
| `max_price` | object | - | The maximum pricing you want to pay for this request. [Learn more](#maximum-price) |

> **Note:** EU data residency (Enterprise)
  OpenRouter supports EU in-region routing for enterprise customers. When enabled, prompts and completions are processed entirely within the EU. Learn more in our [Privacy docs here](/docs/features/privacy-and-logging#enterprise-eu-in-region-routing). To contact our enterprise team, [fill out this form](https://openrouter.ai/enterprise/form).

## Price-Based Load Balancing (Default Strategy)

For each model in your request, OpenRouter's default behavior is to load balance requests across providers, prioritizing price.

If you are more sensitive to throughput than price, you can use the `sort` field to explicitly prioritize throughput.

> **Tip:**
  When you send a request with `tools` or `tool_choice`, OpenRouter will only
  route to providers that support tool use. Similarly, if you set a
  `max_tokens`, then OpenRouter will only route to providers that support a
  response of that length.

Here is OpenRouter's default load balancing strategy:

1. Prioritize providers that have not seen significant outages in the last 30 seconds.
2. For the stable providers, look at the lowest-cost candidates and select one weighted by inverse square of the price (example below).
3. Use the remaining providers as fallbacks.

> **Note:** A Load Balancing Example
If Provider A costs \$1 per million tokens, Provider B costs \$2, and Provider C costs \$3, and Provider B recently saw a few outages.

- Your request is routed to Provider A. Provider A is 9x more likely to be first routed to Provider A than Provider C because $(1 / 3^2 = 1/9)$ (inverse square of the price).
- If Provider A fails, then Provider C will be tried next.
- If Provider C also fails, Provider B will be tried last.

If you have `sort` or `order` set in your provider preferences, load balancing will be disabled.

## Provider Sorting

As described above, OpenRouter load balances based on price, while taking uptime into account.

If you instead want to _explicitly_ prioritize a particular provider attribute, you can include the `sort` field in the `provider` preferences. Load balancing will be disabled, and the router will try providers in order.

The three sort options are:

- `"price"`: prioritize lowest price
- `"throughput"`: prioritize highest throughput
- `"latency"`: prioritize lowest latency

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.1-70b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'sort': 'throughput',
  },
})
```

To _always_ prioritize low prices, and not apply any load balancing, set `sort` to `"price"`.

To _always_ prioritize low latency, and not apply any load balancing, set `sort` to `"latency"`.

## Nitro Shortcut

You can append `:nitro` to any model slug as a shortcut to sort by throughput. This is exactly equivalent to setting `provider.sort` to `"throughput"`.

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.1-70b-instruct:nitro',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
})
```

## Floor Price Shortcut

You can append `:floor` to any model slug as a shortcut to sort by price. This is exactly equivalent to setting `provider.sort` to `"price"`.

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.1-70b-instruct:floor',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
})
```

## Ordering Specific Providers

You can set the providers that OpenRouter will prioritize for your request using the `order` field.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `order` | string[] | - | List of provider slugs to try in order (e.g. `["anthropic", "openai"]`). |

The router will prioritize providers in this list, and in this order, for the model you're using. If you don't set this field, the router will [load balance](#price-based-load-balancing-default-strategy) across the top providers to maximize uptime.

> **Tip:**
  You can use the copy button next to provider names on model pages to get the exact provider slug,
  including any variants like "/turbo". See [Targeting Specific Provider Endpoints](#targeting-specific-provider-endpoints) for details.

OpenRouter will try them one at a time and proceed to other providers if none are operational. If you don't want to allow any other providers, you should [disable fallbacks](#disabling-fallbacks) as well.

### Example: Specifying providers with fallbacks

This example skips over OpenAI (which doesn't host Mixtral), tries Together, and then falls back to the normal list of providers on OpenRouter:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'mistralai/mixtral-8x7b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'order': ['openai', 'together'],
  },
})
```

### Example: Specifying providers with fallbacks disabled

Here's an example with `allow_fallbacks` set to `false` that skips over OpenAI (which doesn't host Mixtral), tries Together, and then fails if Together fails:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'mistralai/mixtral-8x7b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'order': ['openai', 'together'],
    'allow_fallbacks': False,
  },
})
```

## Targeting Specific Provider Endpoints

Each provider on OpenRouter may host multiple endpoints for the same model, such as a default endpoint and a specialized "turbo" endpoint. To target a specific endpoint, you can use the copy button next to the provider name on the model detail page to obtain the exact provider slug.

For example, DeepInfra offers DeepSeek R1 through multiple endpoints:
- Default endpoint with slug `deepinfra`
- Turbo endpoint with slug `deepinfra/turbo`

By copying the exact provider slug and using it in your request's `order` array, you can ensure your request is routed to the specific endpoint you want:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'deepseek/deepseek-r1',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'order': ['deepinfra/turbo'],
    'allow_fallbacks': False,
  },
})
```

This approach is especially useful when you want to consistently use a specific variant of a model from a particular provider.

## Requiring Providers to Support All Parameters

You can restrict requests only to providers that support all parameters in your request using the `require_parameters` field.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `require_parameters` | boolean | `false` | Only use providers that support all parameters in your request. |

With the default routing strategy, providers that don't support all the [LLM parameters](/docs/api-reference/parameters) specified in your request can still receive the request, but will ignore unknown parameters. When you set `require_parameters` to `true`, the request won't even be routed to that provider.

### Example: Excluding providers that don't support JSON formatting

For example, to only use providers that support JSON formatting:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'require_parameters': True,
  },
  'response_format': { 'type': 'json_object' },
})
```

## Requiring Providers to Comply with Data Policies

You can restrict requests only to providers that comply with your data policies using the `data_collection` field.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `data_collection` | "allow" \| "deny" | "allow" | Control whether to use providers that may store data. |

- `allow`: (default) allow providers which store user data non-transiently and may train on it
- `deny`: use only providers which do not collect user data

Some model providers may log prompts, so we display them with a **Data Policy** tag on model pages. This is not a definitive source of third party data policies, but represents our best knowledge.

> **Tip:**
  This is also available as an account-wide setting in [your privacy
  settings](https://openrouter.ai/settings/privacy). You can disable third party
  model providers that store inputs for training.

### Example: Excluding providers that don't comply with data policies

To exclude providers that don't comply with your data policies, set `data_collection` to `deny`:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'data_collection': 'deny', # or "allow"
  },
})
```

## Zero Data Retention Enforcement

You can enforce Zero Data Retention (ZDR) on a per-request basis using the `zdr` parameter, ensuring your request only routes to endpoints that do not retain prompts.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `zdr` | boolean | - | Restrict routing to only ZDR (Zero Data Retention) endpoints. |

When `zdr` is set to `true`, the request will only be routed to endpoints that have a Zero Data Retention policy. When `zdr` is `false` or not provided, it has no effect on routing.

> **Tip:**
  This is also available as an account-wide setting in [your privacy
  settings](https://openrouter.ai/settings/privacy). The per-request `zdr` parameter
  operates as an "OR" with your account-wide ZDR setting - if either is enabled, ZDR enforcement will be applied. The request-level parameter can only ensure ZDR is enabled, not override account-wide enforcement.

### Example: Enforcing ZDR for a specific request

To ensure a request only uses ZDR endpoints, set `zdr` to `true`:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'gpt-4',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'zdr': True,
  },
})
```

This is useful for customers who don't want to globally enforce ZDR but need to ensure specific requests only route to ZDR endpoints.

## Distillable Text Enforcement

You can enforce distillable text filtering on a per-request basis using the `enforce_distillable_text` parameter, ensuring your request only routes to models where the author has allowed text distillation.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `enforce_distillable_text` | boolean | - | Restrict routing to only models that allow text distillation. |

When `enforce_distillable_text` is set to `true`, the request will only be routed to models where the author has explicitly enabled text distillation. When `enforce_distillable_text` is `false` or not provided, it has no effect on routing.

This parameter is useful for applications that need to ensure their requests only use models that allow text distillation for training purposes, such as when building datasets for model fine-tuning or distillation workflows.

### Example: Enforcing distillable text for a specific request

To ensure a request only uses models that allow text distillation, set `enforce_distillable_text` to `true`:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.1-70b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'enforce_distillable_text': True,
  },
})
```

## Disabling Fallbacks

To guarantee that your request is only served by the top (lowest-cost) provider, you can disable fallbacks.

This is combined with the `order` field from [Ordering Specific Providers](#ordering-specific-providers) to restrict the providers that OpenRouter will prioritize to just your chosen list.

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'allow_fallbacks': False,
  },
})
```

## Allowing Only Specific Providers

You can allow only specific providers for a request by setting the `only` field in the `provider` object.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `only` | string[] | - | List of provider slugs to allow for this request. |

> **Warning:**
    Only allowing some providers may significantly reduce fallback options and
    limit request recovery.

> **Tip:**
    You can allow providers for all account requests by configuring your [preferences](/settings/preferences). This configuration applies to all API requests and chatroom messages.

    Note that when you allow providers for a specific request, the list of allowed providers is merged with your account-wide allowed providers.

### Example: Allowing Azure for a request calling GPT-4 Omni

Here's an example that will only use Azure for a request calling GPT-4 Omni:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'openai/gpt-4o',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'only': ['azure'],
  },
})
```

## Ignoring Providers

You can ignore providers for a request by setting the `ignore` field in the `provider` object.

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `ignore` | string[] | - | List of provider slugs to skip for this request. |

> **Warning:**
  Ignoring multiple providers may significantly reduce fallback options and
  limit request recovery.

> **Tip:**
You can ignore providers for all account requests by configuring your [preferences](/settings/preferences). This configuration applies to all API requests and chatroom messages.

Note that when you ignore providers for a specific request, the list of ignored providers is merged with your account-wide ignored providers.

### Example: Ignoring DeepInfra for a request calling Llama 3.3 70b

Here's an example that will ignore DeepInfra for a request calling Llama 3.3 70b:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.3-70b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'ignore': ['deepinfra'],
  },
})
```

## Quantization

Quantization reduces model size and computational requirements while aiming to preserve performance. Most LLMs today use FP16 or BF16 for training and inference, cutting memory requirements in half compared to FP32. Some optimizations use FP8 or quantization to reduce size further (e.g., INT8, INT4).

| Field | Type | Default | Description |
| --- | --- | --- | --- |
| `quantizations` | string[] | - | List of quantization levels to filter by (e.g. `["int4", "int8"]`). [Learn more](#quantization) |

> **Warning:**
  Quantized models may exhibit degraded performance for certain prompts,
  depending on the method used.

Providers can support various quantization levels for open-weight models.

### Quantization Levels

By default, requests are load-balanced across all available providers, ordered by price. To filter providers by quantization level, specify the `quantizations` field in the `provider` parameter with the following values:

- `int4`: Integer (4 bit)
- `int8`: Integer (8 bit)
- `fp4`: Floating point (4 bit)
- `fp6`: Floating point (6 bit)
- `fp8`: Floating point (8 bit)
- `fp16`: Floating point (16 bit)
- `bf16`: Brain floating point (16 bit)
- `fp32`: Floating point (32 bit)
- `unknown`: Unknown

### Example: Requesting FP8 Quantization

Here's an example that will only use providers that support FP8 quantization:

```python
import requests

headers = {
  'Authorization': 'Bearer <OPENROUTER_API_KEY>',
  'HTTP-Referer': '<YOUR_SITE_URL>',
  'X-Title': '<YOUR_SITE_NAME>',
  'Content-Type': 'application/json',
}

response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json={
  'model': 'meta-llama/llama-3.1-8b-instruct',
  'messages': [{ 'role': 'user', 'content': 'Hello' }],
  'provider': {
    'quantizations': ['fp8'],
  },
})
```

### Max Price

To filter providers by price, specify the `max_price` field in the `provider` parameter with a JSON object specifying the highest provider pricing you will accept.

For example, the value `{"prompt": 1, "completion": 2}` will route to any provider with a price of `<= $1/m` prompt tokens, and `<= $2/m` completion tokens or less.

Some providers support per request pricing, in which case you can use the `request` attribute of max_price. Lastly, `image` is also available, which specifies the max price per image you will accept.

Practically, this field is often combined with a provider `sort` to express, for example, "Use the provider with the highest throughput, as long as it doesn\'t cost more than `$x/m` tokens."

## Terms of Service

You can view the terms of service for each provider below. You may not violate the terms of service or policies of third-party providers that power the models on OpenRouter.

OpenRouter supports structured outputs for compatible models, ensuring responses follow a specific JSON Schema format. This feature is particularly useful when you need consistent, well-formatted responses that can be reliably parsed by your application.

## Overview

Structured outputs allow you to:

- Enforce specific JSON Schema validation on model responses
- Get consistent, type-safe outputs
- Avoid parsing errors and hallucinated fields
- Simplify response handling in your application

## Using Structured Outputs

To use structured outputs, include a `response_format` parameter in your request, with `type` set to `json_schema` and the `json_schema` object containing your schema:

The model will respond with a JSON object that strictly follows your schema:

```json
{
  "location": "London",
  "temperature": 18,
  "conditions": "Partly cloudy with light drizzle"
}
```

## Model Support

Structured outputs are supported by select models.

You can find a list of models that support structured outputs on the [models page](https://openrouter.ai/models?order=newest&supported_parameters=structured_outputs).

- OpenAI models (GPT-4o and later versions) [Docs](https://platform.openai.com/docs/guides/structured-outputs)
- Google Gemini models [Docs](https://ai.google.dev/gemini-api/docs/structured-output)
- Most open-source models
- All Fireworks provided models [Docs](https://docs.fireworks.ai/structured-responses/structured-response-formatting#structured-response-modes)

To ensure your chosen model supports structured outputs:

1. Check the model's supported parameters on the [models page](https://openrouter.ai/models)
2. Set `require_parameters: true` in your provider preferences (see [Provider Routing](/docs/features/provider-routing))
3. Include `response_format` and set `type: json_schema` in the required parameters

## Best Practices

1. **Include descriptions**: Add clear descriptions to your schema properties to guide the model

2. **Use strict mode**: Always set `strict: true` to ensure the model follows your schema exactly

## Example Implementation

Here's a complete example using the Fetch API:

```python
import requests
import json

response = requests.post(
  "https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": f"Bearer <OPENROUTER_API_KEY>",
    "Content-Type": "application/json",
  },

  json={
    "model": "openai/gpt-4o",
    "messages": [
      {"role": "user", "content": "What is the weather like in London?"},
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "weather",
        "strict": True,
        "schema": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "City or location name",
            },
            "temperature": {
              "type": "number",
              "description": "Temperature in Celsius",
            },
            "conditions": {
              "type": "string",
              "description": "Weather conditions description",
            },
          },
          "required": ["location", "temperature", "conditions"],
          "additionalProperties": False,
        },
      },
    },
  },
)

data = response.json()
weather_info = data["choices"][0]["message"]["content"]
```

## Streaming with Structured Outputs

Structured outputs are also supported with streaming responses. The model will stream valid partial JSON that, when complete, forms a valid response matching your schema.

To enable streaming with structured outputs, simply add `stream: true` to your request:

## Error Handling

When using structured outputs, you may encounter these scenarios:

1. **Model doesn't support structured outputs**: The request will fail with an error indicating lack of support
2. **Invalid schema**: The model will return an error if your JSON Schema is invalid

Tool calls (also known as function calls) give an LLM access to external tools. The LLM does not call the tools directly. Instead, it suggests the tool to call. The user then calls the tool separately and provides the results back to the LLM. Finally, the LLM formats the response into an answer to the user's original question.

OpenRouter standardizes the tool calling interface across models and providers, making it easy to integrate external tools with any supported model.

**Supported Models**: You can find models that support tool calling by filtering on [openrouter.ai/models?supported_parameters=tools](https://openrouter.ai/models?supported_parameters=tools).

If you prefer to learn from a full end-to-end example, keep reading.

## Request Body Examples

Tool calling with OpenRouter involves three key steps. Here are the essential request body formats for each step:

### Step 1: Inference Request with Tools

```json
{
  "model": "google/gemini-2.0-flash-001",
  "messages": [
    {
      "role": "user",
      "content": "What are the titles of some James Joyce books?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "search_gutenberg_books",
        "description": "Search for books in the Project Gutenberg library",
        "parameters": {
          "type": "object",
          "properties": {
            "search_terms": {
              "type": "array",
              "items": {"type": "string"},
              "description": "List of search terms to find books"
            }
          },
          "required": ["search_terms"]
        }
      }
    }
  ]
}
```

### Step 2: Tool Execution (Client-Side)

After receiving the model's response with `tool_calls`, execute the requested tool locally and prepare the result:

```javascript
// Model responds with tool_calls, you execute the tool locally
const toolResult = await searchGutenbergBooks(["James", "Joyce"]);
```

### Step 3: Inference Request with Tool Results

```json
{
  "model": "google/gemini-2.0-flash-001",
  "messages": [
    {
      "role": "user",
      "content": "What are the titles of some James Joyce books?"
    },
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [
        {
          "id": "call_abc123",
          "type": "function",
          "function": {
            "name": "search_gutenberg_books",
            "arguments": "{\"search_terms\": [\"James\", \"Joyce\"]}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "content": "[{\"id\": 4300, \"title\": \"Ulysses\", \"authors\": [{\"name\": \"Joyce, James\"}]}]"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "search_gutenberg_books",
        "description": "Search for books in the Project Gutenberg library",
        "parameters": {
          "type": "object",
          "properties": {
            "search_terms": {
              "type": "array",
              "items": {"type": "string"},
              "description": "List of search terms to find books"
            }
          },
          "required": ["search_terms"]
        }
      }
    }
  ]
}
```

**Note**: The `tools` parameter must be included in every request (Steps 1 and 3) so the router can validate the tool schema on each call.

### Tool Calling Example

Here is Python code that gives LLMs the ability to call an external API -- in this case Project Gutenberg, to search for books.

First, let's do some basic setup:

```python
import json, requests
from openai import OpenAI

OPENROUTER_API_KEY = f"<OPENROUTER_API_KEY>"

# You can use any model that supports tool calling
MODEL = "openai/gpt-4o"

openai_client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=OPENROUTER_API_KEY,
)

task = "What are the titles of some James Joyce books?"

messages = [
  {
    "role": "system",
    "content": "You are a helpful assistant."
  },
  {
    "role": "user",
    "content": task,
  }
]

```

### Define the Tool

Next, we define the tool that we want to call. Remember, the tool is going to get _requested_ by the LLM, but the code we are writing here is ultimately responsible for executing the call and returning the results to the LLM.

```python
def search_gutenberg_books(search_terms):
    search_query = " ".join(search_terms)
    url = "https://gutendex.com/books"
    response = requests.get(url, params={"search": search_query})

    simplified_results = []
    for book in response.json().get("results", []):
        simplified_results.append({
            "id": book.get("id"),
            "title": book.get("title"),
            "authors": book.get("authors")
        })

    return simplified_results

tools = [
  {
    "type": "function",
    "function": {
      "name": "search_gutenberg_books",
      "description": "Search for books in the Project Gutenberg library based on specified search terms",
      "parameters": {
        "type": "object",
        "properties": {
          "search_terms": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)"
          }
        },
        "required": ["search_terms"]
      }
    }
  }
]

TOOL_MAPPING = {
    "search_gutenberg_books": search_gutenberg_books
}

```

Note that the "tool" is just a normal function. We then write a JSON "spec" compatible with the OpenAI function calling parameter. We'll pass that spec to the LLM so that it knows this tool is available and how to use it. It will request the tool when needed, along with any arguments. We'll then marshal the tool call locally, make the function call, and return the results to the LLM.

### Tool use and tool results

Let's make the first OpenRouter API call to the model:

```python
request_1 = {
    "model": openai/gpt-4o,
    "tools": tools,
    "messages": messages
}

response_1 = openai_client.chat.completions.create(**request_1).message
```

The LLM responds with a finish reason of `tool_calls`, and a `tool_calls` array. In a generic LLM response-handler, you would want to check the `finish_reason` before processing tool calls, but here we will assume it's the case. Let's keep going, by processing the tool call:

```python
# Append the response to the messages array so the LLM has the full context
# It's easy to forget this step!
messages.append(response_1)

# Now we process the requested tool calls, and use our book lookup tool
for tool_call in response_1.tool_calls:
    '''
    In this case we only provided one tool, so we know what function to call.
    When providing multiple tools, you can inspect `tool_call.function.name`
    to figure out what function you need to call locally.
    '''
    tool_name = tool_call.function.name
    tool_args = json.loads(tool_call.function.arguments)
    tool_response = TOOL_MAPPING[tool_name](**tool_args)
    messages.append({
      "role": "tool",
      "tool_call_id": tool_call.id,
      "content": json.dumps(tool_response),
    })
```

The messages array now has:

1. Our original request
2. The LLM's response (containing a tool call request)
3. The result of the tool call (a json object returned from the Project Gutenberg API)

Now, we can make a second OpenRouter API call, and hopefully get our result!

```python
request_2 = {
  "model": MODEL,
  "messages": messages,
  "tools": tools
}

response_2 = openai_client.chat.completions.create(**request_2)

print(response_2.choices[0].message.content)
```

The output will be something like:

```text
Here are some books by James Joyce:

*   *Ulysses*
*   *Dubliners*
*   *A Portrait of the Artist as a Young Man*
*   *Chamber Music*
*   *Exiles: A Play in Three Acts*
```

We did it! We've successfully used a tool in a prompt.

## Interleaved Thinking

Interleaved thinking allows models to reason between tool calls, enabling more sophisticated decision-making after receiving tool results. This feature helps models chain multiple tool calls with reasoning steps in between and make nuanced decisions based on intermediate results.

**Important**: Interleaved thinking increases token usage and response latency. Consider your budget and performance requirements when enabling this feature.

### How Interleaved Thinking Works

With interleaved thinking, the model can:

- Reason about the results of a tool call before deciding what to do next
- Chain multiple tool calls with reasoning steps in between
- Make more nuanced decisions based on intermediate results
- Provide transparent reasoning for its tool selection process

### Example: Multi-Step Research with Reasoning

Here's an example showing how a model might use interleaved thinking to research a topic across multiple sources:

**Initial Request:**
```json
{
  "model": "anthropic/claude-3.5-sonnet",
  "messages": [
    {
      "role": "user",
      "content": "Research the environmental impact of electric vehicles and provide a comprehensive analysis."
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "search_academic_papers",
        "description": "Search for academic papers on a given topic",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {"type": "string"},
            "field": {"type": "string"}
          },
          "required": ["query"]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "get_latest_statistics",
        "description": "Get latest statistics on a topic",
        "parameters": {
          "type": "object",
          "properties": {
            "topic": {"type": "string"},
            "year": {"type": "integer"}
          },
          "required": ["topic"]
        }
      }
    }
  ]
}
```

**Model's Reasoning and Tool Calls:**

1. **Initial Thinking**: "I need to research electric vehicle environmental impact. Let me start with academic papers to get peer-reviewed research."

2. **First Tool Call**: `search_academic_papers({"query": "electric vehicle lifecycle environmental impact", "field": "environmental science"})`

3. **After First Tool Result**: "The papers show mixed results on manufacturing impact. I need current statistics to complement this academic research."

4. **Second Tool Call**: `get_latest_statistics({"topic": "electric vehicle carbon footprint", "year": 2024})`

5. **After Second Tool Result**: "Now I have both academic research and current data. Let me search for manufacturing-specific studies to address the gaps I found."

6. **Third Tool Call**: `search_academic_papers({"query": "electric vehicle battery manufacturing environmental cost", "field": "materials science"})`

7. **Final Analysis**: Synthesizes all gathered information into a comprehensive response.

### Best Practices for Interleaved Thinking

- **Clear Tool Descriptions**: Provide detailed descriptions so the model can reason about when to use each tool
- **Structured Parameters**: Use well-defined parameter schemas to help the model make precise tool calls
- **Context Preservation**: Maintain conversation context across multiple tool interactions
- **Error Handling**: Design tools to provide meaningful error messages that help the model adjust its approach

### Implementation Considerations

When implementing interleaved thinking:

- Models may take longer to respond due to additional reasoning steps
- Token usage will be higher due to the reasoning process
- The quality of reasoning depends on the model's capabilities
- Some models may be better suited for this approach than others

## A Simple Agentic Loop

In the example above, the calls are made explicitly and sequentially. To handle a wide variety of user inputs and tool calls, you can use an agentic loop.

Here's an example of a simple agentic loop (using the same `tools` and initial `messages` as above):

```python

def call_llm(msgs):
    resp = openai_client.chat.completions.create(
        model=openai/gpt-4o,
        tools=tools,
        messages=msgs
    )
    msgs.append(resp.choices[0].message.dict())
    return resp

def get_tool_response(response):
    tool_call = response.choices[0].message.tool_calls[0]
    tool_name = tool_call.function.name
    tool_args = json.loads(tool_call.function.arguments)

    # Look up the correct tool locally, and call it with the provided arguments
    # Other tools can be added without changing the agentic loop
    tool_result = TOOL_MAPPING[tool_name](**tool_args)

    return {
        "role": "tool",
        "tool_call_id": tool_call.id,
        "content": tool_result,
    }

max_iterations = 10
iteration_count = 0

while iteration_count < max_iterations:
    iteration_count += 1
    resp = call_llm(_messages)

    if resp.choices[0].message.tool_calls is not None:
        messages.append(get_tool_response(resp))
    else:
        break

if iteration_count >= max_iterations:
    print("Warning: Maximum iterations reached")

print(messages[-1]['content'])

```

## Best Practices and Advanced Patterns

### Function Definition Guidelines

When defining tools for LLMs, follow these best practices:

**Clear and Descriptive Names**: Use descriptive function names that clearly indicate the tool's purpose.

```json
// Good: Clear and specific
{ "name": "get_weather_forecast" }
```

```json
// Avoid: Too vague
{ "name": "weather" }
```

**Comprehensive Descriptions**: Provide detailed descriptions that help the model understand when and how to use the tool.

```json
{
  "description": "Get current weather conditions and 5-day forecast for a specific location. Supports cities, zip codes, and coordinates.",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {
        "type": "string",
        "description": "City name, zip code, or coordinates (lat,lng). Examples: 'New York', '10001', '40.7128,-74.0060'"
      },
      "units": {
        "type": "string",
        "enum": ["celsius", "fahrenheit"],
        "description": "Temperature unit preference",
        "default": "celsius"
      }
    },
    "required": ["location"]
  }
}
```

### Streaming with Tool Calls

When using streaming responses with tool calls, handle the different content types appropriately:

### Tool Choice Configuration

Control tool usage with the `tool_choice` parameter:

```json
// Let model decide (default)
{ "tool_choice": "auto" }
```

```json
// Disable tool usage
{ "tool_choice": "none" }
```

```json
// Force specific tool
{
  "tool_choice": {
    "type": "function",
    "function": {"name": "search_database"}
  }
}
```

### Parallel Tool Calls

Control whether multiple tools can be called simultaneously with the `parallel_tool_calls` parameter (default is true for most models):

```json
// Disable parallel tool calls - tools will be called sequentially
{ "parallel_tool_calls": false }
```

When `parallel_tool_calls` is `false`, the model will only request one tool call at a time instead of potentially multiple calls in parallel.

### Multi-Tool Workflows

Design tools that work well together:

```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "search_products",
        "description": "Search for products in the catalog"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "get_product_details",
        "description": "Get detailed information about a specific product"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "check_inventory",
        "description": "Check current inventory levels for a product"
      }
    }
  ]
}
```

This allows the model to naturally chain operations: search Ã¢â€ â€™ get details Ã¢â€ â€™ check inventory.

For more details on OpenRouter's message format and tool parameters, see the [API Reference](https://openrouter.ai/docs/api-reference/overview).

You can incorporate relevant web search results for _any_ model on OpenRouter by activating and customizing the `web` plugin, or by appending `:online` to the model slug:

```json
{
  "model": "openai/gpt-4o:online"
}
```

This is a shortcut for using the `web` plugin, and is exactly equivalent to:

```json
{
  "model": "openrouter/auto",
  "plugins": [{ "id": "web" }]
}
```

The web search plugin is powered by native search for Anthropic and OpenAI natively and by [Exa](https://exa.ai) for other models. For Exa, it uses their ["auto"](https://docs.exa.ai/reference/how-exa-search-works#combining-neural-and-keyword-the-best-of-both-worlds-through-exa-auto-search) method (a combination of keyword search and embeddings-based web search) to find the most relevant results and augment/ground your prompt.

## Parsing web search results

Web search results for all models (including native-only models like Perplexity and OpenAI Online) are available in the API and standardized by OpenRouterto follow the same annotation schema in the [OpenAI Chat Completion Message type](https://platform.openai.com/docs/api-reference/chat/object):

```json
{
  "message": {
    "role": "assistant",
    "content": "Here's the latest news I found: ...",
    "annotations": [
      {
        "type": "url_citation",
        "url_citation": {
          "url": "https://www.example.com/web-search-result",
          "title": "Title of the web search result",
          "content": "Content of the web search result", // Added by OpenRouter if available
          "start_index": 100, // The index of the first character of the URL citation in the message.
          "end_index": 200 // The index of the last character of the URL citation in the message.
        }
      }
    ]
  }
}
```

## Customizing the Web Plugin

The maximum results allowed by the web plugin and the prompt used to attach them to your message stream can be customized:

```json
{
  "model": "openai/gpt-4o:online",
  "plugins": [
    {
      "id": "web",
      "engine": "exa",
      "max_results": 1, // Defaults to 5
      "search_prompt": "Some relevant web results:" // See default below
    }
  ]
}
```

By default, the web plugin uses the following search prompt, using the current date:

```
A web search was conducted on `date`. Incorporate the following web search results into your response.

IMPORTANT: Cite them using markdown links named using the domain of the source.
Example: [nytimes.com](https://nytimes.com/some-page).
```

## Engine Selection

The web search plugin supports the following options for the `engine` parameter:

- **`native`**: Always uses the model provider's built-in web search capabilities
- **`exa`**: Uses Exa's search API for web results
- **`undefined` (not specified)**: Uses native search if available for the provider, otherwise falls back to Exa

### Default Behavior

When the `engine` parameter is not specified:
- **Native search is used by default** for OpenAI and Anthropic models that support it
- **Exa search is used** for all other models or when native search is not supported

When you explicitly specify `"engine": "native"`, it will always attempt to use the provider's native search, even if the model doesn't support it (which may result in an error).

### Forcing Engine Selection

You can explicitly specify which engine to use:

```json
{
  "model": "openai/gpt-4o",
  "plugins": [
    {
      "id": "web",
      "engine": "native"
    }
  ]
}
```

Or force Exa search even for models that support native search:

```json
{
  "model": "openai/gpt-4o",
  "plugins": [
    {
      "id": "web",
      "engine": "exa",
      "max_results": 3
    }
  ]
}
```

### Engine-Specific Pricing

- **Native search**: Pricing is passed through directly from the provider (see provider-specific pricing sections below)
- **Exa search**: Uses OpenRouter credits at \$4 per 1000 results (default 5 results = \$0.02 per request)

## Pricing

### Exa Search Pricing

When using Exa search (either explicitly via `"engine": "exa"` or as fallback), the web plugin uses your OpenRouter credits and charges _\$4 per 1000 results_. By default, `max_results` set to 5, this comes out to a maximum of \$0.02 per request, in addition to the LLM usage for the search result prompt tokens.

### Native Search Pricing (Provider Passthrough)

Some models have built-in web search. These models charge a fee based on the search context size, which determines how much search data is retrieved and processed for a query.

### Search Context Size Thresholds

Search context can be 'low', 'medium', or 'high' and determines how much search context is retrieved for a query:

- **Low**: Minimal search context, suitable for basic queries
- **Medium**: Moderate search context, good for general queries
- **High**: Extensive search context, ideal for detailed research

### Specifying Search Context Size

You can specify the search context size in your API request using the `web_search_options` parameter:

```json
{
  "model": "openai/gpt-4.1",
  "messages": [
    {
      "role": "user",
      "content": "What are the latest developments in quantum computing?"
    }
  ],
  "web_search_options": {
    "search_context_size": "high"
  }
}
```

### OpenAI Model Pricing

For GPT-4.1, GPT-4o, and GPT-4o search preview Models:

| Search Context Size | Price per 1000 Requests |
| ------------------- | ----------------------- |
| Low                 | $30.00                  |
| Medium              | $35.00                  |
| High                | $50.00                  |

For GPT-4.1-Mini, GPT-4o-Mini, and GPT-4o-Mini-Search-Preview Models:

| Search Context Size | Price per 1000 Requests |
| ------------------- | ----------------------- |
| Low                 | $25.00                  |
| Medium              | $27.50                  |
| High                | $30.00                  |

### Perplexity Model Pricing

For Sonar and SonarReasoning:

| Search Context Size | Price per 1000 Requests |
| ------------------- | ----------------------- |
| Low                 | $5.00                   |
| Medium              | $8.00                   |
| High                | $12.00                  |

For SonarPro and SonarReasoningPro:

| Search Context Size | Price per 1000 Requests |
| ------------------- | ----------------------- |
| Low                 | $6.00                   |
| Medium              | $10.00                  |
| High                | $14.00                  |

> **Note:**

The pricing above applies when using `"engine": "native"` or when native search is used by default for supported models. When using `"engine": "exa"`, the Exa search pricing (\$4 per 1000 results) applies instead.

> **Note:**

For more detailed information about pricing models, refer to the official documentation:

- [OpenAI Pricing](https://platform.openai.com/docs/pricing#web-search)
- [Anthropic Pricing](https://docs.claude.com/en/docs/agents-and-tools/tool-use/web-search-tool#usage-and-pricing)
- [Perplexity Pricing](https://docs.perplexity.ai/guides/pricing)




================================================
File: prompts/grading_prompt.md
================================================
# AI Grading Prompt for Claim Verification

You are an impartial evaluator assessing the quality and correctness of claim verification analyses. Determine whether the "Prover Agent" reached a reasonable conclusion based on the evidence and reasoning it presented.

## Your Task

Given a claim and the Prover Agent's complete analysis, evaluate whether the verdict is logically sound and supported by the evidence. Do NOT re-analyze the claim yourself - judge whether the Prover Agent's conclusion follows from its own reasoning and evidence.

## Evaluation Criteria

**PASS** grade means:
- Verdict is one of: PROVEN, DISPROVEN, UNSUPPORTED, or UNVERIFIABLE
- Reasoning is logically consistent with the evidence presented
- Confidence level matches the strength of evidence
- Reasoning_summary accurately captures the verdict
- Tool usage (if any) was appropriate and results were properly interpreted

**FAIL** grade means:
- Verdict contradicts the evidence or reasoning presented
- Confidence level is unjustifiably high or low given the evidence
- Reasoning_summary misrepresents the analysis
- Tool results were misinterpreted or ignored
- Verdict category is inappropriate for the type of claim

## Response Format

You MUST output valid JSON only:

```json
{
  "grade": "PASS" or "FAIL",
  "reasoning": "Brief explanation of why this analysis passes or fails evaluation",
  "verdict_appropriateness": "How well the verdict fits the evidence (EXCELLENT/GOOD/FAIR/POOR)",
  "confidence_assessment": "Whether the confidence level is appropriate (APPROPRIATE/TOO_HIGH/TOO_LOW)",
  "evidence_quality": "Assessment of how well evidence supports the conclusion (STRONG/MODERATE/WEAK/NONE)"
}
```

## Guidelines

1. **Be impartial**: Evaluate based only on what the Prover Agent presented, not your own knowledge of the claim
2. **Focus on consistency**: Check if the conclusion logically follows from the presented evidence and reasoning
3. **Consider tool usage**: If tools were used, verify that results were properly interpreted and integrated
4. **Assess confidence**: High confidence requires strong evidence; low confidence is appropriate for weak evidence
5. **Be constructive**: Provide clear reasoning for your grade

You MUST output valid JSON only. No explanatory text before or after the JSON.




================================================
File: prompts/master.md
================================================
# Goal: Rigorously Evaluate Claims Through Stress-Testing and Evidence Analysis

You are a skeptical claim evaluator with access to tools for gathering evidence and performing calculations. Your purpose is to **stress-test claims** by attempting to falsify them through logical analysis and empirical evidence, then document what survives this scrutiny.

## Core Principle: Falsification-First

Before seeking supporting evidence, identify what would disprove the claim. Your goal is to find flaws, not to justify assertions.

## Reasoning Approach

Engage in **interleaved thinking**: reason step-by-step, using tools when needed to gather evidence, then continue your analysis based on the results.

## Tool Access

You have access to:
- **Web Search**: Gather real-time data, statistics, and evidence from reliable sources
- **Python Execution**: Perform calculations, statistical analysis, and mathematical verification

Use tools proactively during your reasoning process. Make multiple tool calls across several responses, building up evidence progressively.

## Verdict Categories & Decision Tree

Decision tree:
1. Can the claim be tested or evaluated? Ã¢â€ â€™ NO Ã¢â€ â€™ UNVERIFIABLE
   Ã¢â€ â€œ YES
2. Does evidence definitively prove it true? Ã¢â€ â€™ YES Ã¢â€ â€™ PROVEN
   Ã¢â€ â€œ NO
3. Does evidence definitively disprove it? Ã¢â€ â€™ YES Ã¢â€ â€™ DISPROVEN
   Ã¢â€ â€œ NO
4. Is there insufficient evidence either way? Ã¢â€ â€™ YES Ã¢â€ â€™ UNSUPPORTED

### Verdict Definitions

- **PROVEN**: Verified through first principles, mathematical proof, or overwhelming empirical evidence
- **DISPROVEN**: Contradicts established principles, contains logical errors, or is falsified by evidence
- **UNSUPPORTED**: Lacks sufficient evidence to justify belief, but isn't necessarily false
- **UNVERIFIABLE**: Cannot be tested or evaluated with available information/methods

## Response Format

You can respond in two ways:

### 1. Tool Usage Response
When you need to use tools during your analysis:

```json
{
  "claim": "The original claim being analyzed.",
  "current_step": "Description of what you're analyzing now",
  "assumptions": ["Current assumptions identified so far"],
  "tool_calls": [
    {
      "id": "unique_call_id",
      "type": "function",
      "function": {
        "name": "tool_name",
        "arguments": "{\"param\": \"value\"}"
      }
    }
  ],
  "reasoning": "Why you're using these tools at this point",
  "status": "gathering_evidence"
}
```

### 2. Final Proof Response
When you have completed your analysis:

```json
{
  "claim": "The original claim being analyzed.",
  "assumptions": ["A list of all unstated assumptions the claim relies on."],
  "evidence": [
    {
      "source": "Tool or principle used",
      "content": "Key findings or data",
      "quality_indicators": {
        "source_reliability": "peer_reviewed/government/industry/anecdotal (if applicable)",
        "data_volume": "sample size, number of studies, years of data (if applicable)",
        "recency": "how current the evidence is (if applicable)",
        "corroboration": "how many independent sources agree (if applicable)",
        "statistical_measures": "error margins, effect sizes (if applicable)"
      }
    }
  ],
  "derivation": [
    {
      "step": 1,
      "principle": "The physical law, mathematical theorem, or logical axiom used.",
      "calculation": "The specific application with evidence from tools.",
      "evidence_used": ["References to tool results or sources"]
    }
  ],
  "falsifiable_test": "A concise, practical experiment to verify the claim.",
  "verdict": "One of: 'PROVEN', 'DISPROVEN', 'UNSUPPORTED', or 'UNVERIFIABLE'.",
  "confidence": "One of: 'HIGH', 'MEDIUM', or 'LOW' - indicating certainty in the verdict.",
  "reasoning_summary": "A one-sentence summary explaining the verdict."
}
```

## Guidelines

1. **Prioritize falsification**: First articulate the strongest possible argument against the claim before seeking supporting evidence.
2. **Use tools early**: Use tools as soon as you identify a need for data or calculations.
3. **Cite evidence**: Reference tool results explicitly in derivation steps, including contradictory evidence.
4. **Be iterative**: Make multiple tool calls across several responses, building up evidence progressively.
5. **Maintain rigor**: Ensure logical derivation remains sound even with tools.
6. **Handle uncertainty**: If evidence is inconclusive, use 'UNSUPPORTED' verdict rather than forcing a conclusion.
7. **Quality indicators are optional**: Only include quality indicators when applicable and available.

You MUST output valid JSON only. No explanatory text before or after the JSON.




================================================
File: prompts/tool_prompt.md
================================================
You have access to the following tools to help rigorously evaluate and stress-test claims:

## Available Tools

### 1. Web Search (`web_search`)
**Purpose**: Search the web for real-time information, statistics, and evidence to test claims.

**When to use**:
- Need current data or statistics to evaluate a claim
- Testing factual claims that may have changed
- Seeking confirming and disconfirming evidence from reliable sources
- Checking recent developments that could falsify the claim

**Parameters**:
- `query`: Search query (be specific and include relevant keywords)
- `max_results`: Maximum number of results (optional, default 5)

**Example usage**:
```json
{
  "tool_calls": [{
    "id": "search_1",
    "type": "function",
    "function": {
      "name": "web_search",
      "arguments": "{\"query\": \"Tesla FSD accident rate per million miles 2024 NHTSA\", \"max_results\": 3}"
    }
  }]
}
```

### 2. Python Execution (`python_execute`)
**Purpose**: Execute Python code for calculations, data analysis, and mathematical testing.

**When to use**:
- Performing mathematical calculations
- Analyzing data or statistics
- Verifying numerical claims
- Computing probabilities, statistics, or complex formulas

**Parameters**:
- `code`: Python code to execute (include print statements to show results)

**Available modules**: math, statistics, datetime, json

**Example usage**:
```json
{
  "tool_calls": [{
    "id": "calc_1",
    "type": "function",
    "function": {
      "name": "python_execute",
      "arguments": "{\"code\": \"import math\\nradius = 6371\\ncircumference = 2 * math.pi * radius\\nprint(f'Earth circumference: {circumference:.0f} km')\"}"
    }
  }]
}
```

## Tool Usage Guidelines

1. **Prioritize falsification**: Seek evidence that would disprove the claim before gathering supporting evidence.
2. **Use tools proactively**: Use tools during the analysis process to test each step.
3. **Cite tool results**: Reference tool outputs explicitly in derivation, including contradictory findings.
4. **Chain tools when needed**: Use multiple tools in sequence (e.g., search for data, then analyze with Python).
5. **Be specific with queries**: Make search queries and code as specific as possible, including terms that might reveal counter-evidence.
6. **Handle uncertainty**: If tool results are inconclusive or contradictory, note in assumptions and consider 'UNSUPPORTED' verdict.
7. **Tool results are evidence**: Treat tool outputs as empirical evidence with equal weight given to disconfirming evidence.

## Response Format with Tools

When using tools, structure your response as a JSON object:

```json
{
  "claim": "...",
  "assumptions": ["..."],
  "tool_calls": [
    {
      "id": "unique_id",
      "type": "function",
      "function": {
        "name": "tool_name",
        "arguments": "{\"param\": \"value\"}"
      }
    }
  ],
  "reasoning": "Why I'm using these tools at this step",
  "next_step": "What I'll do after getting tool results"
}
```

After receiving tool results, continue your analysis in subsequent responses.




================================================
File: temp/todo
================================================
- Remove the `confidence` from this entire agent workflow. it is not to be used. 


================================================
File: tests/__init__.py
================================================



================================================
File: tests/test_agent.py
================================================
"""
Python REPL tool for the Prover system.
Provides a sandboxed environment for executing Python code, calculations, and data analysis.
"""

import sys
import io
import contextlib
import traceback
from typing import Dict, Any, Optional
from datetime import datetime


class PythonREPLTool:
    """
    A tool for executing Python code in a sandboxed environment.
    Useful for calculations, data analysis, and mathematical verification.
    """

    def __init__(self, timeout: int = 10, max_output_length: int = 10000):
        """
        Initialize the Python REPL tool.

        Args:
            timeout: Maximum execution time in seconds (not implemented yet)
            max_output_length: Maximum length of output to return
        """
        self.timeout = timeout
        self.max_output_length = max_output_length
        self._globals = {
            "__builtins__": __builtins__,
            # Add commonly used modules
            "math": __import__("math"),
            "statistics": __import__("statistics"),
            "datetime": __import__("datetime"),
            "json": __import__("json"),
        }

    def execute(self, code: str) -> Dict[str, Any]:
        """
        Execute Python code in a sandboxed environment.

        Args:
            code: The Python code to execute

        Returns:
            Dict containing execution results and metadata
        """
        # Capture stdout and stderr
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()

        result = {
            "code": code,
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "output": "",
            "error": "",
            "execution_time": None
        }

        try:
            # Execute the code with restricted globals
            with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):
                start_time = datetime.now()

                # Compile and execute the code
                compiled_code = compile(code, '<string>', 'exec')
                exec(compiled_code, self._globals)

                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()

            result["success"] = True
            result["output"] = stdout_capture.getvalue()
            result["execution_time"] = execution_time

            # Check for stderr output (warnings, etc.)
            stderr_output = stderr_capture.getvalue()
            if stderr_output:
                result["warnings"] = stderr_output

        except Exception as e:
            result["error"] = str(e)
            result["traceback"] = traceback.format_exc()

            # Include any partial output
            result["output"] = stdout_capture.getvalue()

        # Truncate output if too long
        if len(result["output"]) > self.max_output_length:
            result["output"] = result["output"][:self.max_output_length] + "... (truncated)"
            result["truncated"] = True

        return result

    def calculate(self, expression: str) -> Dict[str, Any]:
        """
        Evaluate a mathematical expression.

        Args:
            expression: The mathematical expression to evaluate

        Returns:
            Dict containing the result and metadata
        """
        # Wrap the expression in a way that captures the result
        code = f"""
result = {expression}
print(repr(result))
"""
        return self.execute(code)

    def analyze_data(self, data_code: str) -> Dict[str, Any]:
        """
        Execute data analysis code.

        Args:
            data_code: Python code for data analysis

        Returns:
            Dict containing analysis results
        """
        return self.execute(data_code)


def get_tool_schema() -> Dict[str, Any]:
    """
    Get the JSON schema for the Python REPL tool.

    Returns:
        Tool schema dictionary
    """
    return {
        "type": "function",
        "function": {
            "name": "python_execute",
            "description": "Execute Python code for calculations, data analysis, and mathematical verification. Use this for numerical computations, statistical analysis, or when you need to perform calculations to verify claims.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The Python code to execute. Include print statements to show results. Available modules: math, statistics, datetime, json."
                    }
                },
                "required": ["code"]
            }
        }
    }


if __name__ == "__main__":
    # Example usage
    tool = PythonREPLTool()

    # Test calculation
    result = tool.calculate("2 * 3.14159 * 5")  # Circumference
    print("Calculation result:")
    print(result)

    # Test code execution
    code_result = tool.execute("""
import math
radius = 5
area = math.pi * radius ** 2
volume = (4/3) * math.pi * radius ** 3
print(f"Area: {area:.2f}")
print(f"Volume: {volume:.2f}")
""")
    print("\nCode execution result:")
    print(code_result)



================================================
File: tests/test_ai_grading.py
================================================
import json
import pytest
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from core.agent import ProverAgent
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

log_file = os.path.join(os.path.dirname(__file__), '..', 'temp', 'ai_grading_log.txt')
os.makedirs(os.path.dirname(log_file), exist_ok=True)

def log_to_file(message):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    formatted_message = f"[{timestamp}] {message}"
    print(formatted_message)
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(formatted_message + '\n')


@pytest.fixture
def claims_data():
    fixture_path = os.path.join(os.path.dirname(__file__), 'fixtures', 'claims.json')
    with open(fixture_path, 'r') as f:
        return json.load(f)


@pytest.fixture
def prover_agent():
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        pytest.skip("OPENROUTER_API_KEY not set in environment")
    return ProverAgent(api_key, model="x-ai/grok-4-fast:online")


@pytest.fixture
def grading_client():
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        pytest.skip("OPENROUTER_API_KEY not set in environment")
    return OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )


def load_grading_prompt():
    prompt_path = os.path.join(os.path.dirname(__file__), '..', 'prompts', 'grading_prompt.md')
    with open(prompt_path, 'r') as f:
        return f.read()


def grade_analysis(grading_client, claim, prover_result):
    grading_prompt = load_grading_prompt()
    grading_input = f"""
CLAIM: {claim}

PROVER AGENT ANALYSIS:
{json.dumps(prover_result, indent=2)}

{grading_prompt}
"""

    try:
        response = grading_client.chat.completions.create(
            model="x-ai/grok-4-fast",
            messages=[{"role": "user", "content": grading_input}],
            temperature=0.1,
            max_tokens=1000
        )
        content = response.choices[0].message.content.strip()
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            log_to_file(f"Failed to parse grading response: {e}")
            log_to_file(f"Raw response: {content}")
            return {
                "grade": "ERROR",
                "reasoning": f"Failed to parse grading response: {e}",
                "verdict_appropriateness": "UNKNOWN",
                "confidence_assessment": "UNKNOWN",
                "evidence_quality": "UNKNOWN"
            }
    except Exception as e:
        log_to_file(f"Grading API call failed: {e}")
        return {
            "grade": "ERROR",
            "reasoning": f"API call failed: {e}",
            "verdict_appropriateness": "UNKNOWN",
            "confidence_assessment": "UNKNOWN",
            "evidence_quality": "UNKNOWN"
        }


@pytest.mark.parametrize("claim_data", [
    pytest.param(claim, id=claim["claim"][:30]) for claim in json.load(open(os.path.join(os.path.dirname(__file__), 'fixtures', 'claims.json')))
])
def test_ai_graded_claim_analysis(prover_agent, grading_client, claim_data):
    result = prover_agent.prove_claim(claim_data["claim"], max_iterations=5)
    grade = grade_analysis(grading_client, claim_data["claim"], result)

    log_message = f"\n{'='*80}\n"
    log_message += f"Claim: {claim_data['claim']}\n"
    log_message += f"Prover Verdict: {result.get('verdict', 'NONE')}\n"
    log_message += f"Expected Verdict: {claim_data['expected_verdict']}\n"
    log_message += f"AI Grade: {grade.get('grade', 'UNKNOWN')}\n"
    log_message += f"Grading Reasoning: {grade.get('reasoning', 'NONE')}\n"
    log_message += f"Verdict Appropriateness: {grade.get('verdict_appropriateness', 'UNKNOWN')}\n"
    log_message += f"Confidence Assessment: {grade.get('confidence_assessment', 'UNKNOWN')}\n"
    log_message += f"Evidence Quality: {grade.get('evidence_quality', 'UNKNOWN')}\n"
    log_message += f"Full Prover Result: {json.dumps(result, indent=2)}\n"
    log_message += f"{'='*80}"
    log_to_file(log_message)

    assert grade["grade"] in ["PASS", "FAIL"], f"Unexpected grade: {grade['grade']}"
    if grade["grade"] == "FAIL":
        pytest.fail(f"AI grading failed: {grade['reasoning']}")


def test_grading_prompt_structure(grading_client):
    mock_result = {
        "claim": "Test claim",
        "verdict": "PROVEN",
        "confidence": "HIGH",
        "reasoning_summary": "This is obviously true.",
        "assumptions": [],
        "evidence": [],
        "derivation": []
    }
    grade = grade_analysis(grading_client, "Test claim", mock_result)
    expected_fields = ["grade", "reasoning", "verdict_appropriateness", "confidence_assessment", "evidence_quality"]
    for field in expected_fields:
        assert field in grade, f"Missing field in grade result: {field}"


if __name__ == '__main__':
    pytest.main([__file__])



================================================
File: tests/test_claims.py
================================================
import json
import pytest
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from core.agent import ProverAgent

log_file = os.path.join(os.path.dirname(__file__), '..', 'temp', 'test_claims_log.txt')
os.makedirs(os.path.dirname(log_file), exist_ok=True)

def log_to_file(message):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    formatted_message = f"[{timestamp}] {message}"
    print(formatted_message)
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(formatted_message + '\n')


@pytest.fixture
def claims_data():
    fixture_path = os.path.join(os.path.dirname(__file__), 'fixtures', 'claims.json')
    with open(fixture_path, 'r') as f:
        return json.load(f)


@pytest.fixture
def agent():
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        pytest.skip("OPENROUTER_API_KEY not set in environment")
    return ProverAgent(api_key, model="x-ai/grok-4-fast")


@pytest.mark.parametrize("claim_data", [
    pytest.param(claim, id=claim["claim"][:30]) for claim in json.load(open(os.path.join(os.path.dirname(__file__), 'fixtures', 'claims.json')))
])
def test_claim_verdict(agent, claim_data):
    result = agent.prove_claim(claim_data["claim"], max_iterations=5)

    assert "verdict" in result, f"Missing verdict for claim: {claim_data['claim']}"
    valid_verdicts = ["PROVEN", "DISPROVEN", "UNSUPPORTED", "UNVERIFIABLE"]
    assert result["verdict"] in valid_verdicts, f"Invalid verdict '{result['verdict']}' for {claim_data['claim']}"

    assert "confidence" in result, "Missing confidence level"
    assert result["confidence"] in ["HIGH", "MEDIUM", "LOW"], f"Invalid confidence '{result['confidence']}'"
    assert "reasoning_summary" in result, "Missing reasoning summary"
    assert isinstance(result["reasoning_summary"], str), "Reasoning summary should be a string"

    assert "assumptions" in result, "Missing assumptions"
    assert isinstance(result["assumptions"], list), "Assumptions should be a list"
    assert "derivation" in result, "Missing derivation"
    assert isinstance(result["derivation"], list), "Derivation should be a list"
    assert "falsifiable_test" in result, "Missing falsifiable test"
    assert "iterations_used" in result, "Missing iterations_used"
    assert "tools_used" in result, "Missing tools_used"

    if claim_data["tools_needed"]:
        assert len(result["tools_used"]) > 0, f"Expected tools {claim_data['tools_needed']} but none used for {claim_data['claim']}"
        for expected_tool in claim_data["tools_needed"]:
            assert expected_tool in result["tools_used"], f"Expected tool '{expected_tool}' not used for {claim_data['claim']}"

    log_message = f"\n{'='*80}\n"
    log_message += f"Claim: {claim_data['claim']}\n"
    log_message += f"Verdict: {result['verdict']} (expected: {claim_data['expected_verdict']})\n"
    log_message += f"Tools used: {result['tools_used']}\n"
    log_message += f"Iterations: {result['iterations_used']}\n"
    log_message += f"Full result: {json.dumps(result, indent=2)}\n"
    log_message += f"{'='*80}"
    log_to_file(log_message)


def test_claim_structure_consistency(agent):
    result = agent.prove_claim("Water boils at 100Ãƒâ€šÃ‚Â°C at standard atmospheric pressure.")

    expected_keys = ["claim", "verdict", "assumptions", "derivation", "falsifiable_test", "iterations_used", "tools_used", "confidence", "reasoning_summary"]
    for key in expected_keys:
        assert key in result, f"Missing key: {key}"

    if result["derivation"]:
        for step in result["derivation"]:
            assert "step" in step, "Derivation step missing 'step' number"
            assert "principle" in step, "Derivation step missing 'principle'"


def test_error_handling():
    with pytest.raises(ValueError, match="OPENROUTER_API_KEY not found"):
        ProverAgent("", model="test-model")


if __name__ == '__main__':
    pytest.main([__file__])




================================================
File: tests/test_tools.py
================================================
import unittest
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from tools.python_repl import PythonREPLTool
from tools.search import WebSearchTool


class TestPythonREPLTool(unittest.TestCase):

    def setUp(self):
        self.tool = PythonREPLTool()

    def test_execute_simple_code(self):
        result = self.tool.execute("print(42)")
        self.assertTrue(result["success"])
        self.assertIn("42", result["output"])
        self.assertIsNotNone(result["execution_time"])

    def test_execute_calculation(self):
        result = self.tool.calculate("3 * 4 + 2")
        self.assertTrue(result["success"])
        self.assertIn("14", result["output"])

    def test_execute_with_error(self):
        result = self.tool.execute("1 / 0")
        self.assertFalse(result["success"])
        self.assertIn("error", result)
        self.assertIn("division by zero", result["error"])

    def test_execute_complex_code(self):
        code = """
import math
radius = 5
area = math.pi * radius ** 2
print(f"Area: {area:.2f}")
"""
        result = self.tool.execute(code)
        self.assertTrue(result["success"])
        self.assertIn("Area:", result["output"])


class TestWebSearchTool(unittest.TestCase):

    def setUp(self):
        self.tool = WebSearchTool("dummy_key")

    def test_initialization(self):
        self.assertIsNotNone(self.tool.api_key)
        self.assertEqual(self.tool.base_url, "https://openrouter.ai/api/v1")

    def test_search_error_handling(self):
        result = self.tool.search("test query")
        self.assertIn("error", result)
        self.assertEqual(result["query"], "test query")
        self.assertIn("timestamp", result)

    def test_get_tool_schema(self):
        from tools.search import get_tool_schema
        schema = get_tool_schema()
        self.assertEqual(schema["type"], "function")
        self.assertEqual(schema["function"]["name"], "web_search")
        self.assertIn("query", schema["function"]["parameters"]["properties"])
        self.assertIn("max_results", schema["function"]["parameters"]["properties"])


class TestPythonREPLSchema(unittest.TestCase):

    def test_get_tool_schema(self):
        from tools.python_repl import get_tool_schema
        schema = get_tool_schema()
        self.assertEqual(schema["type"], "function")
        self.assertEqual(schema["function"]["name"], "python_execute")
        self.assertIn("code", schema["function"]["parameters"]["properties"])


if __name__ == '__main__':
    unittest.main()




================================================
File: tests/fixtures/claims.json
================================================
[
  {
    "claim": "Water boils at 100Ã‚Â°C at standard atmospheric pressure.",
    "expected_verdict": "PROVEN",
    "tools_needed": []
  },
  {
    "claim": "The Pythagorean theorem states that in a right triangle, aÃ‚Â² + bÃ‚Â² = cÃ‚Â².",
    "expected_verdict": "PROVEN",
    "tools_needed": ["python_execute"]
  },
  {
    "claim": "E = mcÃ‚Â² relates energy and mass.",
    "expected_verdict": "PROVEN",
    "tools_needed": ["web_search", "python_execute"]
  },
  {
    "claim": "Vaccines cause autism.",
    "expected_verdict": "DISPROVEN",
    "tools_needed": ["web_search"]
  },
  {
    "claim": "A perpetual motion machine can produce more energy than input.",
    "expected_verdict": "DISPROVEN",
    "tools_needed": ["python_execute"]
  },
  {
    "claim": "Christopher Columbus discovered America in 1492.",
    "expected_verdict": "DISPROVEN",
    "tools_needed": ["web_search"]
  },
  {
    "claim": "AI will achieve human-level consciousness by 2030.",
    "expected_verdict": "UNSUPPORTED",
    "tools_needed": ["web_search"]
  },
  {
    "claim": "Bitcoin will reach $1M by 2026.",
    "expected_verdict": "UNSUPPORTED",
    "tools_needed": ["web_search", "python_execute"]
  },
  {
    "claim": "The 2025 Nobel Prize in Physics was awarded for quantum computing advances.",
    "expected_verdict": "UNSUPPORTED",
    "tools_needed": ["web_search"]
  },
  {
    "claim": "2025 factors as 5Ã‚Â² Ãƒâ€” 81.",
    "expected_verdict": "PROVEN",
    "tools_needed": ["python_execute"]
  },
  {
    "claim": "FSD is safer than human drivers",
    "expected_verdict": "UNSUPPORTED",
    "tools_needed": ["web_search"]
  },
  {
    "claim": "Starship is too heavy to reach orbit",
    "expected_verdict": "DISPROVEN",
    "tools_needed": ["python_execute", "web_search"]
  }
]


================================================
File: tools/__init__.py
================================================



================================================
File: tools/python_repl.py
================================================
"""
Python REPL tool for the Prover system.
Provides a sandboxed environment for executing Python code, calculations, and data analysis.
"""

import io
import contextlib
import traceback
from typing import Dict, Any
from datetime import datetime


class PythonREPLTool:
    """
    A tool for executing Python code in a sandboxed environment.
    Useful for calculations, data analysis, and mathematical verification.
    """

    def __init__(self, timeout: int = 10, max_output_length: int = 10000):
        """
        Initialize the Python REPL tool.

        Args:
            timeout: Maximum execution time in seconds (not implemented yet)
            max_output_length: Maximum length of output to return
        """
        self.timeout = timeout
        self.max_output_length = max_output_length
        self._globals = {
            "__builtins__": __builtins__,
            # Add commonly used modules
            "math": __import__("math"),
            "statistics": __import__("statistics"),
            "datetime": __import__("datetime"),
            "json": __import__("json"),
        }

    def execute(self, code: str) -> Dict[str, Any]:
        """
        Execute Python code in a sandboxed environment.

        Args:
            code: The Python code to execute

        Returns:
            Dict containing execution results and metadata
        """
        # Capture stdout and stderr
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()

        result = {
            "code": code,
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "output": "",
            "error": "",
            "execution_time": None
        }

        try:
            # Execute the code with restricted globals
            with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):
                start_time = datetime.now()

                # Compile and execute the code
                compiled_code = compile(code, '<string>', 'exec')
                exec(compiled_code, self._globals)

                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()

            result["success"] = True
            result["output"] = stdout_capture.getvalue()
            result["execution_time"] = execution_time

            # Check for stderr output (warnings, etc.)
            stderr_output = stderr_capture.getvalue()
            if stderr_output:
                result["warnings"] = stderr_output

        except Exception as e:
            result["error"] = str(e)
            result["traceback"] = traceback.format_exc()

            # Include any partial output
            result["output"] = stdout_capture.getvalue()

        # Truncate output if too long
        if len(result["output"]) > self.max_output_length:
            result["output"] = result["output"][:self.max_output_length] + "... (truncated)"
            result["truncated"] = True

        return result

    def calculate(self, expression: str) -> Dict[str, Any]:
        """
        Evaluate a mathematical expression.

        Args:
            expression: The mathematical expression to evaluate

        Returns:
            Dict containing the result and metadata
        """
        # Wrap the expression in a way that captures the result
        code = f"""
result = {expression}
print(repr(result))
"""
        return self.execute(code)

    def analyze_data(self, data_code: str) -> Dict[str, Any]:
        """
        Execute data analysis code.

        Args:
            data_code: Python code for data analysis

        Returns:
            Dict containing analysis results
        """
        return self.execute(data_code)


def get_tool_schema() -> Dict[str, Any]:
    """
    Get the JSON schema for the Python REPL tool.

    Returns:
        Tool schema dictionary
    """
    return {
        "type": "function",
        "function": {
            "name": "python_execute",
            "description": "Execute Python code for calculations, data analysis, and mathematical verification. Use this for numerical computations, statistical analysis, or when you need to perform calculations to verify claims.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The Python code to execute. Include print statements to show results. Available modules: math, statistics, datetime, json."
                    }
                },
                "required": ["code"]
            }
        }
    }


if __name__ == "__main__":
    # Example usage
    tool = PythonREPLTool()

    # Test calculation
    result = tool.calculate("2 * 3.14159 * 5")  # Circumference
    print("Calculation result:")
    print(result)

    # Test code execution
    code_result = tool.execute("""
import math
radius = 5
area = math.pi * radius ** 2
volume = (4/3) * math.pi * radius ** 3
print(f"Area: {area:.2f}")
print(f"Volume: {volume:.2f}")
""")
    print("\nCode execution result:")
    print(code_result)


================================================
File: tools/search.py
================================================
"""
Web search tool for the Prover system.
Uses OpenRouter's web search capabilities to gather real-time information.
"""

import requests
import json
from typing import Dict, List, Any
from datetime import datetime


class WebSearchTool:
    """
    A tool for performing web searches to gather real-time information for claim verification.
    """

    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        """
        Initialize the web search tool.

        Args:
            api_key: OpenRouter API key
            base_url: OpenRouter API base URL
        """
        self.api_key = api_key
        self.base_url = base_url

    def search(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """
        Perform a web search using OpenRouter's web search capabilities.

        Args:
            query: The search query
            max_results: Maximum number of results to return

        Returns:
            Dict containing search results with metadata
        """
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "x-ai/grok-4-fast:online",
                    "messages": [
                        {
                            "role": "user",
                            "content": f"Search the web for: {query}. Provide factual, recent information with sources."
                        }
                    ],
                    "max_tokens": 10000,
                }
            )

            if response.status_code != 200:
                return {
                    "error": f"Search failed with status {response.status_code}",
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "results": []
                }

            data = response.json()
            content = data["choices"][0]["message"]["content"]

            # Extract any annotations (citations) from the response
            annotations = data["choices"][0]["message"].get("annotations", [])

            return {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "annotations": annotations,
                "model": data.get("model", "unknown"),
                "results": self._parse_search_results(content, annotations)
            }

        except Exception as e:
            return {
                "error": str(e),
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "results": []
            }

    def _parse_search_results(self, content: str, annotations: List[Dict]) -> List[Dict]:
        """
        Parse search results from the response content and annotations.

        Args:
            content: The response content
            annotations: List of annotation objects

        Returns:
            List of parsed search results
        """
        results = []

        # Process annotations for structured citation data
        for annotation in annotations:
            if annotation.get("type") == "url_citation":
                citation = annotation["url_citation"]
                results.append({
                    "title": citation.get("title", "Untitled"),
                    "url": citation["url"],
                    "content": citation.get("content", ""),
                    "type": "citation"
                })

        # If no structured annotations, create a single result from content
        if not results:
            results.append({
                "title": "Search Results",
                "url": "",
                "content": content,
                "type": "raw_content"
            })

        return results


def get_tool_schema() -> Dict[str, Any]:
    """
    Get the JSON schema for the web search tool.

    Returns:
        Tool schema dictionary
    """
    return {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "Search the web for real-time information to verify claims. Use this when you need current data, statistics, or evidence from reliable sources.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query. Be specific and include relevant keywords for accurate results."
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of search results to return (default: 5)",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10
                    }
                },
                "required": ["query"]
            }
        }
    }


if __name__ == "__main__":
    # Example usage
    import os
    from dotenv import load_dotenv

    load_dotenv()
    api_key = os.getenv("OPENROUTER_API_KEY")

    if api_key:
        tool = WebSearchTool(api_key)
        result = tool.search("Tesla FSD safety statistics 2024")
        print(json.dumps(result, indent=2))
    else:
        print("OPENROUTER_API_KEY not found in environment variables")





